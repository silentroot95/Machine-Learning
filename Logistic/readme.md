##### Logistic回归

Logistic回归适用于二分类问题，它把一个线性回归模型，通过激活函数，转化为二分类问题。

广义线性模型
$$
f(x) = w^T\phi_n(x)+b
$$
这里的激活函数选取sigmoid函数，因为它连续可微
$$
y = \frac{1}{1+e^{-z}}
$$
它将z值映射到0到1的区间内，因此它在特定情况下可以理解为概率

将$z = w^Tx+b$代入得
$$
y = \frac{1}{1+e^{-(w^Tx+b)}}
$$
转化
$$
\ln \frac{y}{1-y} = w^Tx+b
$$
将 $y$视为样本$x$为正例得可能性，$1-y$为反例可能性，两者得比值称为几率反映了$x$作为正例得相对可能性。取对数则为对数几率。

将$y$视为类后验概率$p(y=1|x)$，则上式为
$$
\ln \frac{p(y=1|x)}{p(y=0|x)} = w^Tx+b
$$
通过极大似然估计参数$w,b$，似然函数
$$
p(t|w) = \prod_{i=1}^n y_n^{t_n}(1-y_n)^{1-t_n}
$$
其中$t_n$为实际的类别标签，$y_n$为预测的类别标签。

取似然函数的负对数，即交叉熵误差函数
$$
E(w) =-\ln p(t|w)=-\sum_{i=1}^N(t_n\ln y_n+(1-t_n)\ln (1-y_n))
$$
对$w$求导
$$
\nabla E(w) = \sum_{i=1}^N(y_n-t_n)\phi_n
$$
其中$\phi_n$为广义线性模型第n个属性的映射函数。

求解$w$可以采用梯度下降法，牛顿迭代法。对于凸优化问题，梯度下降法可以取得全局最优解，

##### 梯度下降法

梯度下降法是一种批处理方法，一次处理一批数据

迭代公式
$$
w^1 = w^0 -\alpha\nabla E(w) = w^0 -\alpha \sum_{i=1}^n(y_n-t_n)\phi_n
$$
$\alpha$为学习步长。

##### 随机梯度下降法

随机梯度下降法原理与批量梯度梯度下降法相同，只是每次只用一个样本更新系数。这种方法不能很快收敛到最优解，需要对数据集进行多轮迭代。
$$
w^1 = w^0 -\alpha (y_i-t_i)\phi_i
$$




