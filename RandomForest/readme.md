#### 随机森林

了解随机森林之前，先了解一下Bagging

##### Bagging

Bagging是一种经典的并行式集成学习方法，它是基于自助采样的，假设一个包含m个样本的原始数据集，对该数据集进行又放回采样，采样集大小可以自行设定，采样集个数也自行确定。基于每个采样集学习一个基学习器，然后对基学习器进行结合，这就是Bagging的基本流程。

Bagging的思想是基学习器应尽可能相互独立，基学习器相互独立，则要求我们的采样训练集有差异，相互独立。这样才能进行好的集成。

##### 随机森林

随机森林采用Bagging采样方法，以决策树为基学习器，在训练过程中引入随机属性选择，假设共有d个属性传统决策树选取最优属性，随机森林先随机选取k个属性，然后在k个属性中选择最优的。推荐$k=\log_2d$，或$k=\sqrt{d} $.

###### 基学习器

RF采用CART决策树做为基学习器

###### 随机性

特征随机：$k=\log_2d$，或$k=\sqrt{d} $.

样本随机：采样个数与采样比例

###### 特点

由于随机性，可以降低模型方差（variance），故随机森林一般不用做额外剪枝，即可取得较好的泛化能力与抗过拟合能力。当然对于训练集的拟合程度就差一些，也就是偏差（bias）大一些，这也是相对的。

##### CART树

CART树是一个二叉树，每个节点只能分裂出两个分支。

特征选取准则一般有：信息增益，基尼系数。